{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: C:\\Users\\tra1ein\\SharePoint On-Premise\\BT-ASA DataShare - Documents\\006 Log-Files\\PRAESENSA\\PRA-AD604_095340517654530002_Num.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtra1ein\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSharePoint On-Premise\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBT-ASA DataShare - Documents\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m006 Log-Files\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPRAESENSA\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    129\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path\u001b[39m.\u001b[39mhome() \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloads\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moutput.xlsx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m dict_all \u001b[39m=\u001b[39m read_folder(folder_path)\n\u001b[0;32m    131\u001b[0m processed_dict \u001b[39m=\u001b[39m process_dataframes(dict_all)\n\u001b[0;32m    132\u001b[0m write_to_excel(processed_dict, output_file)\n",
      "Cell \u001b[1;32mIn[48], line 76\u001b[0m, in \u001b[0;36mread_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m df_dict:\n\u001b[0;32m     74\u001b[0m     \u001b[39m# If the key already exists in the dictionary, add a new column to the existing dataframe\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     value_name \u001b[39m=\u001b[39m filename\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 76\u001b[0m     df_dict[key][value_name] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     \u001b[39m# If the key does not exist in the dictionary, create a new dataframe\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     df_dict[key] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: filename\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]})\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# def clean_file(filepath):\n",
    "#     # Read the text file\n",
    "#     with open(filepath) as f:\n",
    "#         input_text = f.read()\n",
    "\n",
    "#     # Define the regex pattern & Replace the regex pattern with an empty string\n",
    "#     regex = r'\\r|\\t|\\n|(pd numbers)|([0-9]{2}:[0-9]{2}:[0-9]{2})'\n",
    "#     pattern = re.compile(regex)\n",
    "#     modified_text = pattern.sub('', input_text)\n",
    "\n",
    "#     # Extract the JSON string from the text\n",
    "#     json_string = modified_text[modified_text.index(\"{\"):modified_text.rindex(\"}\")+1]\n",
    "\n",
    "#     # Load the JSON string as a dictionary\n",
    "#     data = json.loads(json_string)\n",
    "\n",
    "#     # Create a DataFrame from the dictionary\n",
    "#     df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "#     df.columns = [\"name\", \"value\"]\n",
    "\n",
    "#     return df\n",
    "def clean_file(filepath):\n",
    "    # Read the text file\n",
    "    with open(filepath) as f:\n",
    "        input_text = f.read()\n",
    "\n",
    "    try:\n",
    "        # Define the regex pattern & Replace the regex pattern with an empty string\n",
    "        regex = r'\\r|\\t|\\n|(pd numbers)|([0-9]{2}:[0-9]{2}:[0-9]{2})'\n",
    "        pattern = re.compile(regex)\n",
    "        modified_text = pattern.sub('', input_text)\n",
    "\n",
    "        # Extract the JSON string from the text\n",
    "        json_string = modified_text[modified_text.index(\"{\"):modified_text.rindex(\"}\")+1]\n",
    "\n",
    "        # Load the JSON string as a dictionary\n",
    "        data = json.loads(json_string)\n",
    "\n",
    "        # Create a DataFrame from the dictionary\n",
    "        df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "        df.columns = [\"name\", \"value\"]\n",
    "\n",
    "        return df\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading file: {filepath}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def read_folder(folder_path):\n",
    "    # Create a dictionary to store the dataframes\n",
    "    df_dict = {}\n",
    "\n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('_Num.txt'):\n",
    "            # Get the key from the filename\n",
    "            key = filename.split('_')[0]\n",
    "\n",
    "            # Clean the file and create the dataframe\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            df = clean_file(filepath)\n",
    "\n",
    "            # Add the dataframe to the dictionary\n",
    "            if key in df_dict:\n",
    "                # If the key already exists in the dictionary, add a new column to the existing dataframe\n",
    "                value_name = filename.split('_')[1].split('.')[0]\n",
    "                df_dict[key][value_name] = df['value']\n",
    "            else:\n",
    "                # If the key does not exist in the dictionary, create a new dataframe\n",
    "                df_dict[key] = df.rename(columns={'value': filename.split('_')[1].split('.')[0]})\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def process_dataframes(df_dict):\n",
    "    # Create a new dictionary to store the processed dataframes\n",
    "    processed_dict = {}\n",
    "\n",
    "    # Loop through each key and dataframe in the input dictionary\n",
    "    for key, df in df_dict.items():\n",
    "        # Transpose the dataframe\n",
    "        df = df.set_index('name')\n",
    "\n",
    "        # Convert the numeric columns to numeric type\n",
    "        numeric_columns = df.columns\n",
    "        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Calculate the number of columns\n",
    "        df[\"Amount of Columns\"] = df[numeric_columns].count(axis=1)\n",
    "\n",
    "        # Calculate the maximum value for each row\n",
    "        df['Max'] = df[numeric_columns].max(axis=1)\n",
    "\n",
    "        # Calculate additional statistics\n",
    "        df[\"Minimum Value (excluding 0)\"] = df[df[numeric_columns] > 0].min(axis=1)\n",
    "        df[\"Average\"] = df[numeric_columns].mean(axis=1)\n",
    "        df[\"Average (excluding 0)\"] = df[df[numeric_columns] > 0].mean(axis=1)\n",
    "        df[\"Standard Deviation\"] = df[numeric_columns].std(axis=1)\n",
    "        df[\"Median\"] = df[numeric_columns].median(axis=1)\n",
    "\n",
    "        # Apply formatting to remove scientific notation and keep 2 decimal places\n",
    "        df = df.applymap(lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "        # Add the processed dataframe to the new dictionary\n",
    "        processed_dict[key] = df\n",
    "\n",
    "    # Return the processed dictionary\n",
    "    return processed_dict\n",
    "\n",
    "\n",
    "def write_to_excel(processed_dict, output_file):\n",
    "    # Export each processed dataframe to a separate sheet in an Excel file\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        for key, df in processed_dict.items():\n",
    "            df.to_excel(writer, sheet_name=key)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = r'C:\\Users\\tra1ein\\SharePoint On-Premise\\BT-ASA DataShare - Documents\\006 Log-Files\\PRAESENSA'\n",
    "output_file = str(Path.home() / \"Downloads\" / \"output.xlsx\")\n",
    "dict_all = read_folder(folder_path)\n",
    "processed_dict = process_dataframes(dict_all)\n",
    "write_to_excel(processed_dict, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tra1ein\\AppData\\Local\\Temp\\ipykernel_15352\\1505921128.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"Standard Deviation\"] = df[numeric_columns].std(axis=1)\n",
      "C:\\Users\\tra1ein\\AppData\\Local\\Temp\\ipykernel_15352\\1505921128.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"Median\"] = df[numeric_columns].median(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error files:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_file(filepath):\n",
    "    # Read the text file\n",
    "    with open(filepath) as f:\n",
    "        input_text = f.read()\n",
    "\n",
    "    # Define the regex pattern & Replace the regex pattern with an empty string\n",
    "    regex = r'\\r|\\t|\\n|\\s|(pd numbers)|([0-9]{2}:[0-9]{2}:[0-9]{2})'\n",
    "    pattern = re.compile(regex)\n",
    "    modified_text = pattern.sub('', input_text)\n",
    "\n",
    "    try:\n",
    "        # Extract the JSON string from the text\n",
    "        json_string = modified_text[modified_text.index(\"{\"):modified_text.rindex(\"}\")+1]\n",
    "\n",
    "        # Load the JSON string as a dictionary\n",
    "        data = json.loads(json_string)\n",
    "\n",
    "        # Create a DataFrame from the dictionary\n",
    "        df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "        df.columns = [\"name\", \"value\"]\n",
    "\n",
    "        return df\n",
    "    except (ValueError, json.JSONDecodeError) as e:\n",
    "        # Print the error message along with the filename\n",
    "        print(f\"Error parsing JSON in file: {filepath}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_folder(folder_path):\n",
    "    # Create a dictionary to store the dataframes\n",
    "    df_dict = {}\n",
    "    error_files = []\n",
    "\n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('_Num.txt'):\n",
    "            # Get the key from the filename\n",
    "            key = filename.split('_')[0]\n",
    "\n",
    "            # Clean the file and create the dataframe\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            df = clean_file(filepath)\n",
    "\n",
    "            # Skip the file if it returns None from clean_file\n",
    "            if df is None:\n",
    "                error_files.append(filename)\n",
    "                continue\n",
    "\n",
    "            # Add the dataframe to the dictionary\n",
    "            if key in df_dict:\n",
    "                # If the key already exists in the dictionary, add a new column to the existing dataframe\n",
    "                value_name = filename.split('_')[1].split('.')[0]\n",
    "                df_dict[key][value_name] = df['value']\n",
    "            else:\n",
    "                # If the key does not exist in the dictionary, create a new dataframe\n",
    "                df_dict[key] = df.rename(columns={'value': filename.split('_')[1].split('.')[0]})\n",
    "\n",
    "    return df_dict, error_files\n",
    "\n",
    "def process_dataframes(df_dict):\n",
    "    # Create a new dictionary to store the processed dataframes\n",
    "    processed_dict = {}\n",
    "\n",
    "    # Loop through each key and dataframe in the input dictionary\n",
    "    for key, df in df_dict.items():\n",
    "\n",
    "        df = df.set_index('name')\n",
    "\n",
    "        # Convert the numeric columns to numeric type\n",
    "        numeric_columns = df.columns\n",
    "        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "        # Calculate the statistics\n",
    "        df[\"Amount of Columns\"] = df[numeric_columns].count(axis=1)\n",
    "        df['Max'] = df[numeric_columns].max(axis=1)\n",
    "        df[\"Minimum Value (excluding 0)\"] = df[df[numeric_columns] > 0].min(axis=1)\n",
    "        df[\"Average\"] = df[numeric_columns].mean(axis=1)\n",
    "        df[\"Average (excluding 0)\"] = df[df[numeric_columns] > 0].mean(axis=1)\n",
    "        df[\"Standard Deviation\"] = df[numeric_columns].std(axis=1)\n",
    "        df[\"Median\"] = df[numeric_columns].median(axis=1)\n",
    "\n",
    "        # Apply formatting to remove scientific notation and keep 2 decimal places\n",
    "        df = df.applymap(lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "        # Add the processed dataframe to the new dictionary\n",
    "        processed_dict[key] = df\n",
    "    return processed_dict\n",
    "\n",
    "folder_path = r'C:\\Users\\tra1ein\\SharePoint On-Premise\\BT-ASA DataShare - Documents\\006 Log-Files\\PRAESENSA'\n",
    "dict_all, error_files = read_folder(folder_path)\n",
    "processed_dict = process_dataframes(dict_all)\n",
    "output_path = r'C:\\Users\\tra1ein\\Downloads\\output.xlsx'\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the error files\n",
    "error_df = pd.DataFrame({\"Error files\": error_files})\n",
    "\n",
    "# Write each processed dataframe to an Excel sheet with the key as sheet name\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    for key, df in processed_dict.items():\n",
    "        df.to_excel(writer, sheet_name=key, index=True)\n",
    "    # Write the error DataFrame to a separate sheet\n",
    "    error_df.to_excel(writer, sheet_name=\"Errors\", index=False)\n",
    "\n",
    "# Print the filenames that encountered errors\n",
    "print(\"Error files:\")\n",
    "print(error_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 name  value\n",
      "0                             AudioDelayCrcErrorCount      0\n",
      "1                                           BootCount     18\n",
      "2               ChannelLoadReference_Ch1_LineA+B_Freq      0\n",
      "3          ChannelLoadReference_Ch1_LineA+B_Impedance      0\n",
      "4    ChannelLoadReference_Ch1_LineA+B_ImpedanceAt1kHz      0\n",
      "..                                                ...    ...\n",
      "176                      RailLimiterAttackCounter_Ch5      0\n",
      "177                      RailLimiterAttackCounter_Ch6      0\n",
      "178                      RailLimiterAttackCounter_Ch7      0\n",
      "179                      RailLimiterAttackCounter_Ch8      0\n",
      "180                                WatchdogResetCount      0\n",
      "\n",
      "[181 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "a = clean_file(r'C:\\Users\\tra1ein\\SharePoint On-Premise\\BT-ASA DataShare - Documents\\006 Log-Files\\PRAESENSA\\PRA-AD604_09534052770653003_Num.txt')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 name   value\n",
      "0                             AudioDelayCrcErrorCount       0\n",
      "1                                           BootCount      31\n",
      "2               ChannelLoadReference_Ch1_LineA+B_Freq    3020\n",
      "3          ChannelLoadReference_Ch1_LineA+B_Impedance    8058\n",
      "4    ChannelLoadReference_Ch1_LineA+B_ImpedanceAt1kHz  137070\n",
      "..                                                ...     ...\n",
      "176                      RailLimiterAttackCounter_Ch5       0\n",
      "177                      RailLimiterAttackCounter_Ch6       0\n",
      "178                      RailLimiterAttackCounter_Ch7       0\n",
      "179                      RailLimiterAttackCounter_Ch8       0\n",
      "180                                WatchdogResetCount       1\n",
      "\n",
      "[181 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "b = clean_file(r'C:\\Users\\tra1ein\\SharePoint On-Premise\\BT-ASA DataShare - Documents\\006 Log-Files\\PRAESENSA\\PRA-AD604_095340517654530002_Num.txt')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     name  045511224022133001  045511224022133006   \n",
      "0  BacklightOperationTime               11235               11461  \\\n",
      "1               BootCount                   8                  12   \n",
      "2               CliAccess          2901277390          2901277390   \n",
      "3    HardwareVersionMajor                   1                   1   \n",
      "4    HardwareVersionMinor                   4                   4   \n",
      "5             LocalVolume          3248488448          3248488448   \n",
      "6           OperationTime               20698               22797   \n",
      "7                   Test1                  42                  42   \n",
      "8                   Test2                 100                 100   \n",
      "9      WatchdogResetCount                   0                   0   \n",
      "\n",
      "   Amount of Columns         Max Minimum Value (excluding 0)       Average   \n",
      "0                  2       11461                     11235.0  1.134800e+04  \\\n",
      "1                  2          12                         8.0  1.000000e+01   \n",
      "2                  2  2901277390                2901277390.0  2.901277e+09   \n",
      "3                  2           1                         1.0  1.000000e+00   \n",
      "4                  2           4                         4.0  4.000000e+00   \n",
      "5                  2  3248488448                3248488448.0  3.248488e+09   \n",
      "6                  2       22797                     20698.0  2.174750e+04   \n",
      "7                  2          42                        42.0  4.200000e+01   \n",
      "8                  2         100                       100.0  1.000000e+02   \n",
      "9                  2           0                        None  0.000000e+00   \n",
      "\n",
      "  Average (excluding 0)  Standard Deviation        Median  \n",
      "0               11348.0          159.806133  1.134800e+04  \n",
      "1                  10.0            2.828427  1.000000e+01  \n",
      "2          2901277390.0            0.000000  2.901277e+09  \n",
      "3                   1.0            0.000000  1.000000e+00  \n",
      "4                   4.0            0.000000  4.000000e+00  \n",
      "5          3248488448.0            0.000000  3.248488e+09  \n",
      "6               21747.5         1484.217134  2.174750e+04  \n",
      "7                  42.0            0.000000  4.200000e+01  \n",
      "8                 100.0            0.000000  1.000000e+02  \n",
      "9                   NaN            0.000000  0.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# test_df = dict_all['PM9-CSLD']\n",
    "\n",
    "# numeric_columns = test_df.columns[1:]\n",
    "# test_df[numeric_columns] = test_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "# # Calculate the number of columns\n",
    "# test_df[\"Amount of Columns\"] = test_df[numeric_columns].count(axis=1)\n",
    "# # Calculate the maximum value for each row\n",
    "# test_df['Max'] = test_df[numeric_columns].max(axis=1)\n",
    "# # Calculate additional statistics\n",
    "# test_df[\"Minimum Value (excluding 0)\"] = test_df[test_df[numeric_columns] > 0].min(axis=1)\n",
    "# test_df[\"Average\"] = test_df[numeric_columns].mean(axis=1)\n",
    "# test_df[\"Average (excluding 0)\"] = test_df[test_df[numeric_columns] > 0].mean(axis=1)\n",
    "# test_df[\"Standard Deviation\"] = test_df[numeric_columns].std(axis=1)\n",
    "# test_df[\"Median\"] = test_df[numeric_columns].median(axis=1)\n",
    "\n",
    "# # Print the resulting DataFrame\n",
    "# print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('anhtruong': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89b9fd202bd36e0516d4d3d058b3cd4b0896bc7dae3157cc97234ac8cd7a569e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
